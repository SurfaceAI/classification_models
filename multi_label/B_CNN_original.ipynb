{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------\n",
    "# Author: Xinqi Zhu\n",
    "# Please cite paper https://arxiv.org/abs/1709.09890 if you use this code\n",
    "#------------------------------------\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "from keras.initializers import he_normal\n",
    "from keras import optimizers\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Learning Rate scheduler\n",
    "def scheduler(epoch):\n",
    "  learning_rate_init = 0.003\n",
    "  if epoch > 40:\n",
    "    learning_rate_init = 0.0005\n",
    "  if epoch > 50:\n",
    "    learning_rate_init = 0.0001\n",
    "  return learning_rate_init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss Weight Modifier\n",
    "class LossWeightsModifier(keras.callbacks.Callback):\n",
    "  def __init__(self, alpha, beta, gamma):\n",
    "    self.alpha = alpha\n",
    "    self.beta = beta\n",
    "    self.gamma = gamma\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if epoch == 8:\n",
    "      K.set_value(self.alpha, 0.1)\n",
    "      K.set_value(self.beta, 0.8)\n",
    "      K.set_value(self.gamma, 0.1)\n",
    "    if epoch == 18:\n",
    "      K.set_value(self.alpha, 0.1)\n",
    "      K.set_value(self.beta, 0.2)\n",
    "      K.set_value(self.gamma, 0.7)\n",
    "    if epoch == 28:\n",
    "      K.set_value(self.alpha, 0)\n",
    "      K.set_value(self.beta, 0)\n",
    "      K.set_value(self.gamma, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------- dimensions ---------\n",
    "img_rows, img_cols = 32, 32\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_rows, img_cols)\n",
    "else:\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "#-----------------------------\n",
    "\n",
    "train_size = 50000\n",
    "\n",
    "#--- coarse 1 classes ---\n",
    "num_c_1 = 2\n",
    "#--- coarse 2 classes ---\n",
    "num_c_2 = 7\n",
    "#--- fine classes ---\n",
    "num_classes  = 10\n",
    "\n",
    "batch_size   = 128\n",
    "epochs       = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--- file paths ---\n",
    "log_filepath = './tb_log_medium_dynamic/'\n",
    "weights_store_filepath = './medium_dynamic_weights/'\n",
    "train_id = '1'\n",
    "model_name = 'weights_medium_dynamic_cifar_10_'+train_id+'.h5'\n",
    "model_path = os.path.join(weights_store_filepath, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [9],\n",
       "       [9],\n",
       "       ...,\n",
       "       [9],\n",
       "       [1],\n",
       "       [1]], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#-------------------- data loading ----------------------\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#---------------- data preprocessiong -------------------\n",
    "x_train = (x_train-np.mean(x_train)) / np.std(x_train)\n",
    "x_test = (x_test-np.mean(x_test)) / np.std(x_test)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#---------------------- make coarse 2 labels --------------------------\n",
    "parent_f = {\n",
    "  2:3, 3:5, 5:5,\n",
    "  1:2, 7:6, 4:6,\n",
    "  0:0, 6:4, 8:1, 9:2\n",
    "}\n",
    "y_c2_train = np.zeros((y_train.shape[0], num_c_2)).astype(\"float32\")\n",
    "y_c2_test = np.zeros((y_test.shape[0], num_c_2)).astype(\"float32\")\n",
    "\n",
    "y_c2_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(y_c2_train.shape[0]):\n",
    "  y_c2_train[i][parent_f[np.argmax(y_train[i])]] = 1.0\n",
    "for i in range(y_c2_test.shape[0]):\n",
    "  y_c2_test[i][parent_f[np.argmax(y_test[i])]] = 1.0\n",
    "\n",
    "#---------------------- make coarse 1 labels --------------------------\n",
    "parent_c2 = {\n",
    "  0:0, 1:0, 2:0,\n",
    "  3:1, 4:1, 5:1, 6:1\n",
    "}\n",
    "y_c1_train = np.zeros((y_c2_train.shape[0], num_c_1)).astype(\"float32\")\n",
    "y_c1_test = np.zeros((y_c2_test.shape[0], num_c_1)).astype(\"float32\")\n",
    "for i in range(y_c1_train.shape[0]):\n",
    "  y_c1_train[i][parent_c2[np.argmax(y_c2_train[i])]] = 1.0\n",
    "for i in range(y_c1_test.shape[0]):\n",
    "  y_c1_test[i][parent_c2[np.argmax(y_c2_test[i])]] = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------------------- model definition ---------------------------\n",
    "alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(1024, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu', name='fc2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(input=img_input, output=[c_1_pred, c_2_pred, fine_pred], name='medium_dynamic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(lr=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "cbks = [change_lr, tb_cb, change_lw]\n",
    "\n",
    "model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "# The following compile() is just a behavior to make sure this model can be saved.\n",
    "# We thought it may be a bug of Keras which cannot save a model compiled with loss_weights parameter\n",
    "#---------------------------------------------------------------------------------\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            # optimizer=keras.optimizers.Adadelta(),\n",
    "            optimizer=sgd, \n",
    "            metrics=['accuracy'])\n",
    "\n",
    "score = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "model.save(model_path)\n",
    "print('score is: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---------------------- make coarse 2 labels --------------------------\n",
    "parent_f = {\n",
    "  2:3, 3:5, 5:5,\n",
    "  1:2, 7:6, 4:6,\n",
    "  0:0, 6:4, 8:1, 9:2\n",
    "}\n",
    "y_c2_train = np.zeros((y_train.shape[0], num_c_2)).astype(\"float32\")\n",
    "y_c2_test = np.zeros((y_test.shape[0], num_c_2)).astype(\"float32\")\n",
    "for i in range(y_c2_train.shape[0]):\n",
    "  y_c2_train[i][parent_f[np.argmax(y_train[i])]] = 1.0\n",
    "for i in range(y_c2_test.shape[0]):\n",
    "  y_c2_test[i][parent_f[np.argmax(y_test[i])]] = 1.0\n",
    "\n",
    "#---------------------- make coarse 1 labels --------------------------\n",
    "parent_c2 = {\n",
    "  0:0, 1:0, 2:0,\n",
    "  3:1, 4:1, 5:1, 6:1\n",
    "}\n",
    "y_c1_train = np.zeros((y_c2_train.shape[0], num_c_1)).astype(\"float32\")\n",
    "y_c1_test = np.zeros((y_c2_test.shape[0], num_c_1)).astype(\"float32\")\n",
    "for i in range(y_c1_train.shape[0]):\n",
    "  y_c1_train[i][parent_c2[np.argmax(y_c2_train[i])]] = 1.0\n",
    "for i in range(y_c1_test.shape[0]):\n",
    "  y_c1_test[i][parent_c2[np.argmax(y_c2_test[i])]] = 1.0\n",
    "\n",
    "\n",
    "#----------------------- model definition ---------------------------\n",
    "alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "\n",
    "img_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "#--- block 1 ---\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "#--- block 2 ---\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "#--- coarse 1 branch ---\n",
    "c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "c_1_bch = BatchNormalization()(c_1_bch)\n",
    "c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)\n",
    "\n",
    "#--- block 3 ---\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "#--- coarse 2 branch ---\n",
    "c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_bch = Dense(512, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "c_2_bch = BatchNormalization()(c_2_bch)\n",
    "c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)\n",
    "\n",
    "#--- block 4 ---\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "#--- fine block ---\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(1024, activation='relu', name='fc_cifar10_1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu', name='fc2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)\n",
    "\n",
    "model = Model(input=img_input, output=[c_1_pred, c_2_pred, fine_pred], name='medium_dynamic')\n",
    "\n",
    "#----------------------- compile and fit ---------------------------\n",
    "sgd = optimizers.SGD(lr=0.003, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=sgd, \n",
    "              loss_weights=[alpha, beta, gamma],\n",
    "              # optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "change_lw = LossWeightsModifier(alpha, beta, gamma)\n",
    "cbks = [change_lr, tb_cb, change_lw]\n",
    "\n",
    "model.fit(x_train, [y_c1_train, y_c2_train, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=cbks,\n",
    "          validation_data=(x_test, [y_c1_test, y_c2_test, y_test]))\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "# The following compile() is just a behavior to make sure this model can be saved.\n",
    "# We thought it may be a bug of Keras which cannot save a model compiled with loss_weights parameter\n",
    "#---------------------------------------------------------------------------------\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            # optimizer=keras.optimizers.Adadelta(),\n",
    "            optimizer=sgd, \n",
    "            metrics=['accuracy'])\n",
    "\n",
    "score = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)\n",
    "model.save(model_path)\n",
    "print('score is: ', score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SurfaceAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
